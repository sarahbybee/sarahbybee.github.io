<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Sarah" />
    
    <link rel="shortcut icon" type="image/x-icon" href="../../img/favicon.ico">
    <title>Modeling, Inference, Prediction</title>
    <meta name="generator" content="Hugo 0.79.0" />
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="../../css/main.css" />
    <link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,400,200bold,400old" />
    
    <!--[if lt IE 9]>
			<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
			<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
		<![endif]-->

    
  </head>

  <body>
    <div id="wrap">
      
      <nav class="navbar navbar-default">
  <div class="container">
    <div class="navbar-header">
      <a class="navbar-brand" href="../../"><i class="fa fa-home"></i></a>
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <div class="navbar-collapse collapse" id="navbar">
      <ul class="nav navbar-nav navbar-right">
      
        
        <li><a href="../../post/">BLOG</a></li>
        
        <li><a href="../../projects/">PROJECTS</a></li>
        
        <li><a href="../../resume/">RESUME</a></li>
        
      
      </ul>
    </div>
  </div>
</nav>

      <div class="container">
        <div class="blog-post">
          <h3>
            <strong><a href="../../project/project2/">Modeling, Inference, Prediction</a></strong>
          </h3>
        </div>
 
<div class="blog-title">
          <h4>
         December 7, 2020 
            &nbsp;&nbsp;
            
          </h4>
        </div>

        <div class="panel panel-default">
          <div class="panel-body">
            <div class="blogpost">
              


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>I downloaded my goodreads library information to use as my dataset. There are 421 observations, each representing a different book.</p>
<pre class="r"><code>library(readr)
goodreadsdata &lt;- read_csv(&quot;goodreadsdata.csv&quot;, 
    col_types = cols(avgrating = col_number(), 
        myrating = col_number(), pages = col_number(), 
        pubyear = col_number()))
goodreads &lt;- goodreadsdata
glimpse(goodreads)</code></pre>
<pre><code>## Rows: 421
## Columns: 7
## $ title &lt;chr&gt; &quot;Charlotte&#39;s Web&quot;, &quot;The Shining&quot;, &quot;The
Complete Sherlock Holmes&quot;, &quot;Will My Cat …
## $ myrating &lt;dbl&gt; 0, 0, 0, 4, 4, 4, 3, 2, 5, 3, 3, 2, 3,
3, 2, 3, 5, 4, 3, 4, 4, 2, 4, 3, 4, 2, 3…
## $ avgrating &lt;dbl&gt; 4.18, 4.23, 4.48, 4.15, 4.13, 4.18,
4.39, 3.54, 4.43, 4.40, 4.36, 4.17, 4.06, 4…
## $ binding &lt;chr&gt; &quot;Paperback&quot;, &quot;Paperback&quot;, &quot;Hardcover&quot;,
&quot;ebook&quot;, &quot;Paperback&quot;, &quot;Audiobook&quot;, &quot;eboo…
## $ pages &lt;dbl&gt; 184, 659, 1077, 237, 225, 257, 203, 563,
315, 219, 208, 220, 208, 224, 78, 256,…
## $ pubyear &lt;dbl&gt; 1952, 1977, 1927, 2019, 1993, 2019,
2012, 2006, 2012, 2011, 2010, 1983, 2002, 2…
## $ shelf &lt;chr&gt; &quot;currently-reading&quot;, &quot;currently-reading&quot;,
&quot;currently-reading&quot;, &quot;read&quot;, &quot;read&quot;, …</code></pre>
<p>The &quot;title&quot; variable contains book titles for each book I have added to my goodreads library. The &quot;myrating&quot; variable is the star rating I gave the book on a scale of 0-5. The &quot;avgrating&quot; variable lists the average rating for the book based on all community ratings. The &quot;binding&quot; variable lists whether a book is a paperback, hardcover, ebook, or audiobook. The &quot;pages&quot; variable is a count of the number of pages in the book. The &quot;pubyear&quot; variable lists the year the book was initially published. The &quot;shelf&quot; variable states whether I have read, want to read, or am currently reading the book.</p>
</div>
<div id="manova" class="section level2">
<h2>MANOVA</h2>
<pre class="r"><code>group &lt;- goodreads$binding
dvs &lt;- goodreads %&gt;% select(avgrating, pages, pubyear)

sapply(split(dvs, group), mshapiro_test)</code></pre>
<pre><code>## Audiobook ebook Hardcover Paperback
## statistic 0.6648097 0.4515287 0.5372794 0.4616239
## p.value 4.742278e-05 9.812528e-15 6.981628e-20
2.827215e-23</code></pre>
<pre class="r"><code>man &lt;- manova(cbind(avgrating, pages, pubyear)~binding, data = goodreads)
summary(man)</code></pre>
<pre><code>## Df Pillai approx F num Df den Df Pr(&gt;F)
## binding 3 0.14632 7.1273 9 1251 3.981e-10 ***
## Residuals 417
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1</code></pre>
<pre class="r"><code>summary.aov(man)</code></pre>
<pre><code>## Response avgrating :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## binding 3 0.2339 0.077952 1.0698 0.3617
## Residuals 417 30.3865 0.072869
##
## Response pages :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## binding 3 727430 242477 10.956 6.129e-07 ***
## Residuals 417 9229067 22132
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Response pubyear :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## binding 3 66053 22018 8.7616 1.202e-05 ***
## Residuals 417 1047915 2513
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1</code></pre>
<pre class="r"><code>pairwise.t.test(goodreads$pages, goodreads$binding, p.adj=&quot;none&quot;)</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  goodreads$pages and goodreads$binding 
## 
##           Audiobook ebook   Hardcover
## ebook     0.91921   -       -        
## Hardcover 0.00209   2e-07   -        
## Paperback 0.03226   0.00026 0.02489  
## 
## P value adjustment method: none</code></pre>
<pre class="r"><code>pairwise.t.test(goodreads$pubyear, goodreads$binding, p.adj=&quot;none&quot;)</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  goodreads$pubyear and goodreads$binding 
## 
##           Audiobook ebook   Hardcover
## ebook     0.7053    -       -        
## Hardcover 0.4142    0.4607  -        
## Paperback 0.0099    9.3e-05 5.6e-05  
## 
## P value adjustment method: none</code></pre>
<pre class="r"><code>1 - .95^16</code></pre>
<pre><code>## [1] 0.5598733</code></pre>
<pre class="r"><code>.05/(1+3+6+6)</code></pre>
<pre><code>## [1] 0.003125</code></pre>
<p>A one-way MANOVA was conducted to determine the effect of book binding (paperback, hardcover, ebook, audiobook) on the three dependent variables average rating, page number, and publication year. The numeric variable quantifying my rating was not included because for all books I have not read the rating is zero and therefore largely dependent on whether or not I have read the book.</p>
<p>After testing for multivariate normality for each group, we reject the null hypothesis that the assumptions were met since each p value is less than 0.0001. Therefore, the assumptions were violated.</p>
<p>The MANOVA showed significant differences among book bindings for at least one of the dependent variables (Pillai trace = 0.15, pseudo F(91251) = 7.13, p &lt; 0.0001).</p>
<p>The univariate ANOVAS were significant for the dependent variables page number and publication year (F(3,147) = 10.96, p &lt; 0.0001 and F(3,147) = 8.76, p &lt; 0.0001).</p>
<p>Post hoc pairwise t tests were done to determine which bindings differed in number of pages and publication year. For number of pages there is a significant difference between audiobooks and hardcovers (p = 0.002), ebooks and hardcovers (p &lt; 0.0001), and ebooks and paperbacks (p = 0.0002). For publication year there is a significant difference between ebooks and paperbacks (p &lt; 0.0001) and hardcovers and paperbacks (p &lt; 0.0001). Since 16 tests were performed, the probability of having at least one type I error is 0.56 and the significance level was adjusted (bonferroni α = .05/16 = 0.0031).</p>
</div>
<div id="randomization-test" class="section level2">
<h2>Randomization Test</h2>
<pre class="r"><code>nocurrentread &lt;- goodreads %&gt;% filter(shelf != &quot;currently-reading&quot;)

nocurrentread %&gt;% group_by(shelf) %&gt;% summarize(means = mean(pages)) %&gt;% summarize(meandiff = diff(means))</code></pre>
<pre><code>## # A tibble: 1 x 1
##   meandiff
##      &lt;dbl&gt;
## 1     10.2</code></pre>
<pre class="r"><code>randomdist &lt;- vector()
for(i in 1:5000){
  new &lt;- data.frame(shelf = nocurrentread$shelf, pages = sample(nocurrentread$pages))
  randomdist[i] &lt;- mean(new[new$shelf==&quot;to-read&quot;,]$pages)-mean(new[new$shelf==&quot;read&quot;,]$pages)}

mean(randomdist&gt;10.175 | randomdist &lt; -10.175)</code></pre>
<pre><code>## [1] 0.4972</code></pre>
<pre class="r"><code>{hist(randomdist, main=&quot;&quot;, ylab = &quot;&quot;); abline(v = c(10.175, -10.175), col = &quot;red&quot;)}</code></pre>
<p><img src="../../project/project2_files/figure-html/unnamed-chunk-3-1.png" width="768" style="display: block; margin: auto;" /> For my randomization test, I measured whether there is a difference between the number of pages in the books I have read versus the books I want to read. The null hypothesis is that the mean page number for read and to-read books is the same. The alternative hypothesis is that the mean page number for read and to-read books is different. After conducting a randomized t test, we fail to reject the null hypothesis and conclude that there is not a significant difference in mean page number between read books and to-read books.</p>
</div>
<div id="linear-regression" class="section level2">
<h2>Linear Regression</h2>
<pre class="r"><code>readbooks &lt;- goodreads %&gt;% filter(shelf == &quot;read&quot;) %&gt;% mutate(avgrate_cat = ifelse(avgrating&gt;mean(avgrating), &quot;good&quot;, &quot;bad&quot;))
readbooks$pages_c &lt;- readbooks$pages - mean(readbooks$pages)

fit &lt;- (lm(myrating ~ pages_c*avgrate_cat, data = readbooks))
summary(fit)</code></pre>
<pre><code>##
## Call:
## lm(formula = myrating ~ pages_c * avgrate_cat, data =
readbooks)
##
## Residuals:
## Min 1Q Median 3Q Max
## -2.60461 -0.49539 -0.00844 0.69225 1.88374
##
## Coefficients:
## Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 3.3053468 0.0846055 39.068 &lt; 2e-16 ***
## pages_c 0.0015075 0.0006633 2.273 0.0239 *
## avgrate_catgood 0.6708708 0.1157660 5.795 1.97e-08 ***
## pages_c:avgrate_catgood 0.0008680 0.0007989 1.087 0.2783
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Residual standard error: 0.9038 on 260 degrees of
freedom
## Multiple R-squared: 0.2531, Adjusted R-squared: 0.2444
## F-statistic: 29.36 on 3 and 260 DF, p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>ggplot(readbooks, aes(x=pages_c, y=myrating, group=avgrate_cat))+
  geom_point(aes(color=avgrate_cat))+
  geom_smooth(method = &quot;lm&quot;, formula=y~1, se=F, fullrange=T, aes(color = avgrate_cat))</code></pre>
<p><img src="../../project/project2_files/figure-html/unnamed-chunk-4-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>resids &lt;- fit$residuals
fitvals &lt;- fit$fitted.values
ggplot() + geom_point(aes(fitvals, resids)) + 
  geom_hline(yintercept = 0, color=&#39;red&#39;)</code></pre>
<p><img src="../../project/project2_files/figure-html/unnamed-chunk-4-2.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ks.test(resids, &quot;pnorm&quot;, mean = 0, sd(resids))</code></pre>
<pre><code>## 
##  One-sample Kolmogorov-Smirnov test
## 
## data:  resids
## D = 0.071534, p-value = 0.1341
## alternative hypothesis: two-sided</code></pre>
<pre class="r"><code>coeftest(fit, vcov = vcovHC(fit))</code></pre>
<pre><code>##
## t test of coefficients:
##
## Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 3.30534680 0.09408869 35.1301 &lt; 2.2e-16 ***
## pages_c 0.00150747 0.00067268 2.2410 0.02587 *
## avgrate_catgood 0.67087083 0.12281657 5.4624 1.098e-07
***
## pages_c:avgrate_catgood 0.00086800 0.00077702 1.1171
0.26499
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1</code></pre>
<p>For my linear regression, I created a new variable that characterizes books as &quot;good&quot; or &quot;bad&quot; based on the average community rating. Books with community ratings above the mean are &quot;good&quot; and books with community ratings below the mean are &quot;bad&quot;.</p>
<p>The prediction for my rating of a bad book with a mean number of pages is 3.305 stars. For every one page increase in page length, my predicted rating for bad books increases by 0.002 stars. For books with an average number of pages, my rating for good books is 0.671 times greater than my rating for bad books. The slope of page number on my rating for good books is 0.0009 greater than my rating for bad books.</p>
<p>My data fails the assumption of linearity and homoscedasticity, but meets the assumption of normality.</p>
<p>After the robust standard error was applied to the model, an increase is seen in standard error for each coefficient estimate. However, mean centered page number and category remain a significant predictor of my rating.</p>
<p>The model explains 24.4% of the variation in my rating.</p>
</div>
<div id="linear-regression-with-bootstrapped-standard-errors" class="section level2">
<h2>Linear Regression with Bootstrapped Standard Errors</h2>
<pre class="r"><code>boot_sample &lt;- replicate(5000, {
  boot_data &lt;- sample_frac(readbooks, replace = T)
  boot_fit &lt;- lm(myrating ~ pages_c*avgrate_cat, data = boot_data)
  coef(boot_fit)
})

boot_sample %&gt;% t %&gt;% as.data.frame %&gt;% summarize_all(sd)</code></pre>
<pre><code>## (Intercept) pages_c avgrate_catgood
pages_c:avgrate_catgood
## 1 0.09200352 0.0006681957 0.1215096 0.0007676447</code></pre>
<p>After computing bootstrapped standard errors by resampling observations, some differences are seen between original, robust, and bootstrapped standard error. Compared to the original standard error, the bootstrapped standard error for the intercept and a good community rating is larger, while the bootstrapped standard error for page number and the interaction between page number and a good rating is smaller. Compared to the robust standard error, the bootstrapped standard error for the intercept, page number, and interaction between page number and good community rating is smaller, while the bootstrapped standard error for good rating is slightly larger.</p>
<p>Overall, the standard errors are pretty similar and likely reflect similar p values and levels of significance.</p>
</div>
<div id="logistic-regression" class="section level2">
<h2>Logistic Regression</h2>
<pre class="r"><code>goodreads &lt;- goodreads %&gt;% mutate(y = ifelse(shelf==&quot;read&quot;, 1, 0))

logfit &lt;- glm(y~avgrating+pubyear, data = goodreads, family = &quot;binomial&quot;)
coeftest(logfit)</code></pre>
<pre><code>##
## z test of coefficients:
##
## Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) -6.67090799 4.09769646 -1.6280 0.1035322
## avgrating 1.32249984 0.38632938 3.4232 0.0006188 ***
## pubyear 0.00091563 0.00192373 0.4760 0.6340971
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1</code></pre>
<pre class="r"><code>exp(coef(logfit))</code></pre>
<pre><code>## (Intercept)   avgrating     pubyear 
## 0.001267248 3.752791039 1.000916051</code></pre>
<pre class="r"><code>goodreads$prob &lt;- predict(logfit, type = &quot;response&quot;)
goodreads$truth &lt;- as.factor(goodreads$y)

table(predict = as.numeric(goodreads$prob &gt; .5), truth = goodreads$truth) %&gt;% addmargins</code></pre>
<pre><code>##        truth
## predict   0   1 Sum
##     0    11  16  27
##     1   146 248 394
##     Sum 157 264 421</code></pre>
<pre class="r"><code>class_diag(goodreads$prob, goodreads$truth)</code></pre>
<pre><code>## acc sens spec ppv f1 auc
## 1 0.6152019 0.9393939 0.07006369 0.6294416 0.7537994
0.6050232</code></pre>
<pre class="r"><code>goodreads$logit &lt;- predict(logfit, type = &quot;link&quot;)

ggplot(goodreads, aes(logit, color = truth, fill = truth)) +
  geom_density(alpha = .5) +
  geom_vline(xintercept = 0)</code></pre>
<p><img src="../../project/project2_files/figure-html/unnamed-chunk-6-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>roc &lt;- ggplot(goodreads) + geom_roc(aes(d=y, m=prob), n.cuts = 0)
roc</code></pre>
<p><img src="../../project/project2_files/figure-html/unnamed-chunk-6-2.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>calc_auc(roc)</code></pre>
<pre><code>##   PANEL group       AUC
## 1     1    -1 0.6050232</code></pre>
<p>The intercept shows that the odds of me having read a book when average community rating and publication year are equal to 0 is 0.001. Controlling for publication year, for every one unit increase in average community rating, the odds of me having read a book increase by a factor of 3.75 (p &lt; 0.001). Controlling for average community rating, for every one unit increase in publication year, the odds of me having read a book increase by 1.00.</p>
<p>The model's accuracy shows that it correctly classifies books 61.52% of the time. The sensitivity is 0.939, representing the probability of predicting a book being read if I have actually read it. The specificity is 0.070, representing the probability of predicting a book is listed as to-read or currently reading for books I have not read. The precision is 0.629, the proportion of books classified as read that actually are. The AUC is 0.605, showing the model is a poor predictor of whether or not I have read a book.</p>
</div>
<div id="logistic-regression-with-all-variables" class="section level2">
<h2>Logistic regression with all variables</h2>
<pre class="r"><code>goodreads_l &lt;- goodreads %&gt;% select(-title, -prob, -truth, -logit, -shelf, -myrating)

logfit2 &lt;- glm(y~(.)^2, data = goodreads_l, family = &quot;binomial&quot;)
coef(logfit2)</code></pre>
<pre><code>## (Intercept) avgrating bindingebook
## 5.273962e+02 -1.145836e+02 -6.747193e+01
## bindingHardcover bindingPaperback pages
## -5.611845e+00 -4.952336e+01 -1.243562e-01
## pubyear avgrating:bindingebook
avgrating:bindingHardcover
## -2.513479e-01 1.754892e+00 8.431789e-01
## avgrating:bindingPaperback avgrating:pages
avgrating:pubyear
## 2.232264e+00 3.786133e-03 5.636227e-02
## bindingebook:pages bindingHardcover:pages
bindingPaperback:pages
## -8.784525e-03 2.782085e-03 2.299393e-03
## bindingebook:pubyear bindingHardcover:pubyear
bindingPaperback:pubyear
## 2.371834e-02 -7.685745e-03 1.188145e-02
## pages:pubyear
## 5.364442e-05</code></pre>
<pre class="r"><code>probs2 &lt;- predict(logfit2, type = &quot;response&quot;)
class_diag(probs2, goodreads_l$y)</code></pre>
<pre><code>## acc sens spec ppv f1 auc
## 1 0.6674584 0.7840909 0.4713376 0.7137931 0.7472924
0.7348967</code></pre>
<p>The accuracy of the model, or the proportion of the time it correctly classifies books, is 0.667. The sensitivity is 0.784, showing the probability of predicting a book being read if I have actually read it. The specificity of the model is 0.471, showing it has a low probability of correctly predicting a book as unread when I have not read it. The precision is 0.714, which is the proportion of books classified as read that actually are. The AUC improved to 0.735, showing the model is a fair predictor of whether or not I have read a book.</p>
<pre class="r"><code>set.seed(1234)
k=10

data &lt;- goodreads_l[sample(nrow(goodreads_l)),]
folds &lt;- cut(seq(1:nrow(goodreads_l)), breaks = k, labels = F)

diags &lt;- NULL
for(i in 1:k){
  train &lt;- data[folds != i,]
  test &lt;- data[folds == i,]
  truth &lt;- test$y
  fit &lt;- glm(y~(.)^2, data = train, family = &quot;binomial&quot;)
  probs &lt;- predict(fit, newdata = test, type = &quot;response&quot;)
  diags &lt;- rbind(diags, class_diag(probs, truth))
}

summarize_all(diags, mean)</code></pre>
<pre><code>## acc sens spec ppv f1 auc
## 1 0.643577 0.7655557 0.4320906 0.6982225 0.7275427
0.6951346</code></pre>
<p>The AUC dropped to 0.581 in the out of sample analysis, making it a bad model of classification. This is worse than the in sample analysis which had an AUC of 0.735.</p>
<pre class="r"><code>y &lt;- as.matrix(goodreads_l$y)
x &lt;- model.matrix(y~(.)^2, data = goodreads_l)[,-1]

cv &lt;- cv.glmnet(x,y,family=&quot;binomial&quot;)
lasso &lt;- glmnet(x,y,family = &quot;binomial&quot;, lambda = cv$lambda.1se)
coef(lasso)</code></pre>
<pre><code>## 19 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                                       s0
## (Intercept)                -3.4601699214
## avgrating                   0.3993958557
## bindingebook                0.2798837690
## bindingHardcover            .           
## bindingPaperback           -0.4344203699
## pages                       .           
## pubyear                     .           
## avgrating:bindingebook      .           
## avgrating:bindingHardcover  .           
## avgrating:bindingPaperback  .           
## avgrating:pages             .           
## avgrating:pubyear           0.0003701437
## bindingebook:pages         -0.0015048803
## bindingHardcover:pages      .           
## bindingPaperback:pages      .           
## bindingebook:pubyear        .           
## bindingHardcover:pubyear   -0.0005645884
## bindingPaperback:pubyear    .           
## pages:pubyear               .</code></pre>
<pre class="r"><code>goodreads_l &lt;- goodreads_l %&gt;% mutate(paperback = ifelse(goodreads_l$binding == &quot;Paperback&quot;, 1, 0))
goodreads_l &lt;- goodreads_l %&gt;% mutate(ebook = ifelse(goodreads_l$binding == &quot;ebook&quot;, 1, 0))
goodreads_l &lt;- goodreads_l %&gt;% mutate(hardcover = ifelse(goodreads_l$binding == &quot;Hardcover&quot;, 1, 0))

set.seed(1234)
k=10

data &lt;- goodreads_l %&gt;% sample_frac
folds &lt;- ntile(1:nrow(data), n = 10)

diags &lt;- NULL
for(i in 1:k){
  train &lt;- data[folds != i,]
  test &lt;- data[folds == i,]
  truth &lt;- test$y
  fit &lt;- glm(y~avgrating+paperback+avgrating:pubyear+ebook:pages+hardcover:pubyear+ebook:pubyear, data = train, family = &quot;binomial&quot;)
  probs &lt;- predict(fit, newdata = test, type = &quot;response&quot;)
  diags &lt;- rbind(diags, class_diag(probs, truth))
}

summarize_all(diags, mean)</code></pre>
<pre><code>## acc sens spec ppv f1 auc
## 1 0.6504983 0.8331249 0.3374742 0.6844112 0.7484011
0.6894647</code></pre>
<p>The variables retained by lasso tend to fluctuate, but those most frequently retained are average community rating, paperback binding, the interaction between average community rating and publication year, the interaction between ebook binding and number of pages, the interaction between ebook binding and publication year, and the interaction between hardcover binding and publication year.</p>
<p>The AUC of the 10 fold CV using the lasso variables is 0.689, so it is a poor model. This AUC is worse than the AUC using all variables (0.735), but better than the AUC for the 10 fold CV using all variables (0.581).</p>
</div>

            
        <hr>         <div class="related-posts">
                <h5>Related Posts</h5>
                
              </div> 
            </div>
          </div>

   <hr>  <div class="disqus">
  <div id="disqus_thread"></div>
  <script type="text/javascript">

    (function() {
      
      
      if (window.location.hostname == "localhost")
        return;

      var disqus_shortname = '';
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div> 
        </div>
      </div>
    </div>

    
    <footer>
  <div id="footer">
    <div class="container">
      <p class="text-muted">&copy; All rights reserved. Powered by <a href="https://gohugo.io/">Hugo</a> and
      <a href="http://www.github.com/nurlansu/hugo-sustain/">sustain</a> with ♥</p>
    </div>
  </div>
</footer>
<div class="footer"></div>


<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="../../js/docs.min.js"></script>
<script src="../../js/main.js"></script>

<script src="../../js/ie10-viewport-bug-workaround.js"></script>


    
  </body>
</html>
